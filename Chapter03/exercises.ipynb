{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Finite MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - The Agent-Environment Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.1</b> \n",
    "</p>\n",
    "\n",
    "Devise three example tasks of your own that fit into the MDP framework,\n",
    "identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "\n",
    "+ Example 1: MMO game. Each player is an agent. The actions might be any movement a player can make in such a game, such as moving forward, backward, attacking, opening inventory, etc. The states are everything that represents the player in its environment: the position of the agent, the enemies around, the obstacles, the HP, etc. The rewards might be given if a player gains XP or complete a quest, negative rewards when it looses HP.\n",
    "\n",
    "+ Example 2: Government policy creation. RL here is applied to determine new policies for a government. The actions might be what to put in each of those policies. This could be laws, specific decisions that will then be applied by the society. The states represents the current set of policies applied, the happiness of the citizens, GDP, trade & diplomatic status with other countries, etc. The rewards might be moment-to-moment measures of key metrics such as employment, GDP, happiness, airquality, etc. \n",
    "\n",
    "+ Example 3: Surgery. RL can be applied to perform surgery with extreme levels of precisions. Depending on the level of abstraction of the agent, the actions can be which tool to pick, which operation to perform. The states are the current location of the patient, tools, the health levels & metrics of the patient. The rewards can be +1 if the operation is successfull. The agent could receive negative rewards for risky actions or anything that destabilize the patient's health. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.2</b> \n",
    "</p>\n",
    "\n",
    "Is the MDP framework adequate to usefully represent all goal-directed\n",
    "learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "Here are some limitations: \n",
    "\n",
    "* Continuous state and action space problems. In this case, it's impossible for tabular methods to represent all values, and new methods need to be used\n",
    "* Non-markovian environment: Environments where the next state depends on previous states sequences, such as stock trading \n",
    "* Any tasks where the goal can't be clear and measurable enough this might be an issue. Also, systems where there are contradictory goals, this requires to find some sort of equilibrium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.3</b> \n",
    "</p>\n",
    "\n",
    "Consider the problem of driving. You could define the actions in terms of\n",
    "the accelerator, steering wheel, and brake, that is, where your body meets the machine.\n",
    "Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive.\n",
    "What is the right level, the right place to draw the line between agent and environment?\n",
    "On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "It's important to chose a level where we can easily map actions with real life functions and where we can actually do something. Then, it's a matter of balance to find the level that enables us to maximize our rewards.\n",
    "\n",
    "The level of which actions are defined should align with the goal for the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.4</b> \n",
    "</p>\n",
    "\n",
    "Give a table analogous to that in Example 3.3, but for $p(s'\n",
    ", r|s, a)$. It should have columns for $s, a, s', r,$ and $p(s', r|s, a)$, and a row for every 4-tuple for which $p(s', r|s, a) > 0.$\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc|c}\n",
    "\\hline\n",
    "\\textbf{s} & \\textbf{a} & \\textbf{s'} & \\textbf{r} & \\textbf{p(s', r|s,a)}\\\\\n",
    "\\hline\n",
    "high & search & high & r_{search} & \\alpha \\\\ \n",
    "\\hline\n",
    "high & search & low & r_{search} & 1-\\alpha \\\\\n",
    "\\hline\n",
    "low & search & high & -3 & 1 - \\beta \\\\\n",
    "\\hline\n",
    "low & search & low & r_{search} & \\beta \\\\\n",
    "\\hline\n",
    "high & wait & high & r_{wait} & 1 \\\\\n",
    "\\hline\n",
    "low & wait & low & r_{wait} & 1 \\\\\n",
    "\\hline\n",
    "low & recharge & high & 0 & 1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Goals and Rewards \n",
    "## 3.3 - Returns and Episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.5</b> \n",
    "</p>\n",
    "\n",
    "The equations in Section 3.1 are for the continuing case and need to be\n",
    "modified (very slightly) to apply to episodic tasks. Show that you know the modification needed by giving the modified version of (3.3). \n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "In this modified version, in a terminal state, there is no action to be taken, no reward, and no next step. But, in the state $s$ prior to a terminal state (which is still within $S$), the next state set needs to include the terminal state, hence: \n",
    "\n",
    "$$\n",
    "\\sum_{s' \\in \\mathcal{S}^+} \\sum_{r \\in \\mathcal R} p(s',r \\mid s, a) = 1, \\quad \\text{for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.6</b> \n",
    "</p>\n",
    "\n",
    "Suppose you treated pole-balancing as an episodic task but also used\n",
    "discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing\n",
    "formulation of this task?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an episodic task, the return $G_t$ is a finite sum of rewards: \n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} +\\dots+\\gamma^{T-t-1}R_T\n",
    "$$\n",
    "\n",
    "All rewards being 0 except for the reward of episode at time T being -1, we can write: \n",
    "\n",
    "$$\n",
    "G_t = -\\gamma^{T-t-1}\n",
    "$$\n",
    "\n",
    "The difference with a continuous task lies in the fact that it would keep accumulating rewards. In a episodic task, the reward gets collected, and we start again at time step 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.7</b> \n",
    "</p>\n",
    "\n",
    "Imagine that you are designing a robot to run a maze. You decide to give it a\n",
    "reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "You are not penalizing the agent for spending time in the maze. As long as the agent end up getting out of the maze, it receives the same reward for one episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.8</b> \n",
    "</p>\n",
    "\n",
    "Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R1 = -1, R2 = 2, R3 = 6, R4 = 3, \\text{and } R5 = 2, \\text{with } T = 5.$ What are $G0, G1, ..., G5$? Hint: Work backwards\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G_5 = 0$ because $T=5$\n",
    "\n",
    "$G_4 = R_5 + \\gamma G_5 = 2$\n",
    "\n",
    "$G_3 = R_4 + \\gamma G_4 = 4$\n",
    "\n",
    "$G_2 = R_3 + \\gamma G_3 = 8$\n",
    "\n",
    "$G_1 = R_2 + \\gamma G_2 = 6$\n",
    "\n",
    "$G_0 = R_1 + \\gamma G_1 = 2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.9</b> \n",
    "</p>\n",
    "\n",
    "Suppose $\\gamma = 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite sequence of 7s. What are $G_1$ and $G_0$? \n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "G_1 = \\sum_{k=0}^{\\infty} 0.9^k*7 = \\frac{7}{1-0.9} = 70 \\\\\n",
    "G_0 = R_1 + \\gamma G_1 = 2 + 0.9*70 = 65\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.10</b> \n",
    "</p>\n",
    "\n",
    "Prove the second equality in (3.10).\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.10 is: \n",
    "$$\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma} \\tag{3.10}\n",
    "$$\n",
    "\n",
    "Let's start with: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_t &= \\sum_{k=0}^{\\infty} \\gamma^k \\\\\n",
    "&= \\frac{(\\sum_{k=0}^{\\infty} \\gamma^k)*(1-\\gamma)}{1-\\gamma} \\\\\n",
    "&= \\frac{1-\\gamma+\\gamma-\\gamma^2+\\gamma^2-\\gamma^3+\\gamma^2-\\gamma^4+\\dots-\\gamma^{\\infty}}{1-\\gamma} \\\\\n",
    "&=\\frac{1-\\cancel{\\gamma}+\\cancel{\\gamma}-\\cancel{\\gamma^2}+\\cancel{\\gamma^2}-\\cancel{\\gamma^3}+\\cancel{\\gamma^3}-\\cancel{\\gamma^4}+\\dots-\\gamma^{\\infty}}{1-\\gamma} \\\\\n",
    "&= \\frac{1-\\gamma^{\\infty}}{1-\\gamma} \\\\\n",
    "\n",
    "\\text{because }\\gamma < 1 : \\\\\n",
    "G_t &= \\frac{1}{1-\\gamma}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Unified Notation for Episodic and Continuing Tasks\n",
    "## 3.5 - Policies and Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.11</b> \n",
    "</p>\n",
    "\n",
    "If the current state is $S_t$, and actions are selected according to stochastic\n",
    "policy $\\pi$, then what is the expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument function p (3.2)?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}[R_{t+1}|S_t = s] = \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|a,s)*r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.12</b> \n",
    "</p>\n",
    "\n",
    "Give an equation for $v_{\\pi}$ in terms of $q_{\\pi}$ and $\\pi$\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[G_t|S_t=s] \\\\\n",
    "&= \\mathbb{E}_{\\pi}[\\sum_{a}\\pi(a,s)q_{\\pi}(a,s)|S_t=s]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.13</b> \n",
    "</p>\n",
    "\n",
    "Give an equation for $q_{\\pi}$ in terms of $v_{\\pi}$ and the four-argument p\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-action value is the sum over all possibilities of next states and rewards given a state s and action a, each discounted by the value-function of the next state. Formally:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi}(s,a) &= \\sum_{s',r}p(s',r|a,s)[r +  \\gamma v_{\\pi}(s')]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.14</b> \n",
    "</p>\n",
    "\n",
    "The Bellman equation (3.14) must hold for each state for the value function\n",
    "$v_{\\pi}$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, -0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this policy, each action is taken randomly, therefore with 4 actions each action has a probability of being taken of 0.25. $\\gamma = 0.9$. Let's call $s_c$ the centered state. \n",
    "\n",
    "The environment is deterministic so each action in a state leads to a deterministic state s'.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_{\\pi}(s_c) &= \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|a,s)[r+\\gamma v_{\\pi}(s')] \\\\\n",
    "&= 0.25*[0+0.9*0.7] + 0.25*[0+0.9*2.3] +\\\\\n",
    "& 0.25*[0+0.9*0.4] + 0.25*[0+0.9*(-0.4)] \\\\\n",
    "&= 0.675 \\approx 0.7\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.15</b> \n",
    "</p>\n",
    "\n",
    "In the gridworld example, rewards are positive for goals, negative for\n",
    "running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant $c$ to all the rewards adds a constant, $v_c$, to the values of all states, and thus does not affect the relative values of any states under any policies. What is $v_c$ in terms\n",
    "of $c$ and $\\gamma$ ?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G_t = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}$\n",
    "\n",
    "By introducing a constant c: \n",
    "\n",
    "$G_{t'} = \\sum_{k=0}^{\\infty}\\gamma^k (R_{t+k+1}+c) = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1} + \\sum_{k=0}^{\\infty}\\gamma^k*c = G_t + \\frac{c}{1-\\gamma}$\n",
    "\n",
    "If we introduce that constant into $v_{\\pi}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v'_{\\pi}(s) &= \\mathbb{E}[G_t+\\frac{c}{1-\\gamma}|S_t=s] \\\\\n",
    "&= \\mathbb{E}[G_t|S_t = s] + \\frac{c}{1-\\gamma} \\\\\n",
    "&= v_{\\pi}(s) + v_c \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore all state values are increased by the constant $v_c$ which makes no relative difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.16</b> \n",
    "</p>\n",
    "\n",
    "Now consider adding a constant c to all the rewards in an episodic task,\n",
    "such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For episodic tasks we have: \n",
    "\n",
    "$G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1}R_k$\n",
    "\n",
    "By introducing a constant c for each reward: \n",
    "\n",
    "$G_t' = \\sum_{k=t+1}^{T} \\gamma^{k-t-1}(R_k+c) = G_t + \\sum_{k=t+1}^{T} \\gamma^{k-t-1}c$\n",
    "\n",
    "The term on the right is not a constant, because it depends on T being a finite number. \n",
    "\n",
    "Therefore, the expression of $v_{\\pi}(s)$ is now: \n",
    "\n",
    "$v'_{\\pi}(s) = \\mathbb{E}[G_t+\\sum_{k=t+1}^{T} \\gamma^{k-t-1}c|S_t=s]$\n",
    "\n",
    "Depending on the sign and magnitude of c, it will either push the agent to keep running in loops collecting big rewards without ever exiting the maze, or push it to exit the maze as fast as possible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.17</b> \n",
    "</p>\n",
    "\n",
    "What is the Bellman equation for action values, that is, for $q_{\\pi}$? It must give the action value $q_{\\pi}(s, a)$ in terms of the action\n",
    "values, $q_{\\pi}(s', a')$, of possible successors to the state–action pair (s, a). Hint: The backup diagram to the right corresponds to this equation.\n",
    "Show the sequence of equations analogous to (3.14), but for action\n",
    "values.\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi}(s,a) &= \\mathbb{E_{\\pi}}[G_t|A_t=a, S_t=s] \\\\\n",
    "&= \\mathbb{E_{\\pi}}[R_{t+1} + \\gamma G_{t+1}|A_t=a, S_t=s] \\\\\n",
    "&= \\sum_{s',r}p(s',r|s,a)[r + \\gamma \\sum_{a'}\\pi(a'|s')q_{\\pi}(s',a')]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
