{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Finite MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - The Agent-Environment Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.1</b> \n",
    "</p>\n",
    "\n",
    "Devise three example tasks of your own that fit into the MDP framework,\n",
    "identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "\n",
    "+ Example 1: MMO game. Each player is an agent. The actions might be any movement a player can make in such a game, such as moving forward, backward, attacking, opening inventory, etc. The states are everything that represents the player in its environment: the position of the agent, the enemies around, the obstacles, the HP, etc. The rewards might be given if a player gains XP or complete a quest, negative rewards when it looses HP.\n",
    "\n",
    "+ Example 2: Government policy creation. RL here is applied to determine new policies for a government. The actions might be what to put in each of those policies. This could be laws, specific decisions that will then be applied by the society. The states represents the current set of policies applied, the happiness of the citizens, GDP, trade & diplomatic status with other countries, etc. The rewards might be moment-to-moment measures of key metrics such as employment, GDP, happiness, airquality, etc. \n",
    "\n",
    "+ Example 3: Surgery. RL can be applied to perform surgery with extreme levels of precisions. Depending on the level of abstraction of the agent, the actions can be which tool to pick, which operation to perform. The states are the current location of the patient, tools, the health levels & metrics of the patient. The rewards can be +1 if the operation is successfull. The agent could receive negative rewards for risky actions or anything that destabilize the patient's health. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.2</b> \n",
    "</p>\n",
    "\n",
    "Is the MDP framework adequate to usefully represent all goal-directed\n",
    "learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "Here are some limitations: \n",
    "\n",
    "* Continuous state and action space problems. In this case, it's impossible for tabular methods to represent all values, and new methods need to be used\n",
    "* Non-markovian environment: Environments where the next state depends on previous states sequences, such as stock trading \n",
    "* Any tasks where the goal can't be clear and measurable enough this might be an issue. Also, systems where there are contradictory goals, this requires to find some sort of equilibrium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
