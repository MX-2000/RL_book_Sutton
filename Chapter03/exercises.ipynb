{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Finite MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - The Agent-Environment Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.1</b> \n",
    "</p>\n",
    "\n",
    "Devise three example tasks of your own that fit into the MDP framework,\n",
    "identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "\n",
    "+ Example 1: MMO game. Each player is an agent. The actions might be any movement a player can make in such a game, such as moving forward, backward, attacking, opening inventory, etc. The states are everything that represents the player in its environment: the position of the agent, the enemies around, the obstacles, the HP, etc. The rewards might be given if a player gains XP or complete a quest, negative rewards when it looses HP.\n",
    "\n",
    "+ Example 2: Government policy creation. RL here is applied to determine new policies for a government. The actions might be what to put in each of those policies. This could be laws, specific decisions that will then be applied by the society. The states represents the current set of policies applied, the happiness of the citizens, GDP, trade & diplomatic status with other countries, etc. The rewards might be moment-to-moment measures of key metrics such as employment, GDP, happiness, airquality, etc. \n",
    "\n",
    "+ Example 3: Surgery. RL can be applied to perform surgery with extreme levels of precisions. Depending on the level of abstraction of the agent, the actions can be which tool to pick, which operation to perform. The states are the current location of the patient, tools, the health levels & metrics of the patient. The rewards can be +1 if the operation is successfull. The agent could receive negative rewards for risky actions or anything that destabilize the patient's health. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.2</b> \n",
    "</p>\n",
    "\n",
    "Is the MDP framework adequate to usefully represent all goal-directed\n",
    "learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "Here are some limitations: \n",
    "\n",
    "* Continuous state and action space problems. In this case, it's impossible for tabular methods to represent all values, and new methods need to be used\n",
    "* Non-markovian environment: Environments where the next state depends on previous states sequences, such as stock trading \n",
    "* Any tasks where the goal can't be clear and measurable enough this might be an issue. Also, systems where there are contradictory goals, this requires to find some sort of equilibrium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.3</b> \n",
    "</p>\n",
    "\n",
    "Consider the problem of driving. You could define the actions in terms of\n",
    "the accelerator, steering wheel, and brake, that is, where your body meets the machine.\n",
    "Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive.\n",
    "What is the right level, the right place to draw the line between agent and environment?\n",
    "On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "It's important to chose a level where we can easily map actions with real life functions and where we can actually do something. Then, it's a matter of balance to find the level that enables us to maximize our rewards.\n",
    "\n",
    "The level of which actions are defined should align with the goal for the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.4</b> \n",
    "</p>\n",
    "\n",
    "Give a table analogous to that in Example 3.3, but for $p(s'\n",
    ", r|s, a)$. It should have columns for $s, a, s', r,$ and $p(s', r|s, a)$, and a row for every 4-tuple for which $p(s', r|s, a) > 0.$\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc|c}\n",
    "\\hline\n",
    "\\textbf{s} & \\textbf{a} & \\textbf{s'} & \\textbf{r} & \\textbf{p(s', r|s,a)}\\\\\n",
    "\\hline\n",
    "high & search & high & r_{search} & \\alpha \\\\ \n",
    "\\hline\n",
    "high & search & low & r_{search} & 1-\\alpha \\\\\n",
    "\\hline\n",
    "low & search & high & -3 & 1 - \\beta \\\\\n",
    "\\hline\n",
    "low & search & low & r_{search} & \\beta \\\\\n",
    "\\hline\n",
    "high & wait & high & r_{wait} & 1 \\\\\n",
    "\\hline\n",
    "low & wait & low & r_{wait} & 1 \\\\\n",
    "\\hline\n",
    "low & recharge & high & 0 & 1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Goals and Rewards \n",
    "## 3.3 - Returns and Episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.5</b> \n",
    "</p>\n",
    "\n",
    "The equations in Section 3.1 are for the continuing case and need to be\n",
    "modified (very slightly) to apply to episodic tasks. Show that you know the modification needed by giving the modified version of (3.3). \n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "In this modified version, in a terminal state, there is no action to be taken, no reward, and no next step. But, in the state $s$ prior to a terminal state (which is still within $S$), the next state set needs to include the terminal state, hence: \n",
    "\n",
    "$$\n",
    "\\sum_{s' \\in \\mathcal{S}^+} \\sum_{r \\in \\mathcal R} p(s',r \\mid s, a) = 1, \\quad \\text{for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.6</b> \n",
    "</p>\n",
    "\n",
    "Suppose you treated pole-balancing as an episodic task but also used\n",
    "discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing\n",
    "formulation of this task?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an episodic task, the return $G_t$ is a finite sum of rewards: \n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} +\\dots+\\gamma^{T-t-1}R_T\n",
    "$$\n",
    "\n",
    "All rewards being 0 except for the reward of episode at time T being -1, we can write: \n",
    "\n",
    "$$\n",
    "G_t = -\\gamma^{T-t-1}\n",
    "$$\n",
    "\n",
    "The difference with a continuous task lies in the fact that it would keep accumulating rewards. In a episodic task, the reward gets collected, and we start again at time step 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.7</b> \n",
    "</p>\n",
    "\n",
    "Imagine that you are designing a robot to run a maze. You decide to give it a\n",
    "reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "You are not penalizing the agent for spending time in the maze. As long as the agent end up getting out of the maze, it receives the same reward for one episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.8</b> \n",
    "</p>\n",
    "\n",
    "Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R1 = -1, R2 = 2, R3 = 6, R4 = 3, \\text{and } R5 = 2, \\text{with } T = 5.$ What are $G0, G1, ..., G5$? Hint: Work backwards\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G_5 = 0$ because $T=5$\n",
    "\n",
    "$G_4 = R_5 + \\gamma G_5 = 2$\n",
    "\n",
    "$G_3 = R_4 + \\gamma G_4 = 4$\n",
    "\n",
    "$G_2 = R_3 + \\gamma G_3 = 8$\n",
    "\n",
    "$G_1 = R_2 + \\gamma G_2 = 6$\n",
    "\n",
    "$G_0 = R_1 + \\gamma G_1 = 2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.9</b> \n",
    "</p>\n",
    "\n",
    "Suppose $\\gamma = 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite sequence of 7s. What are $G_1$ and $G_0$? \n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "G_1 = \\sum_{k=0}^{\\infty} 0.9^k*7 = \\frac{7}{1-0.9} = 70 \\\\\n",
    "G_0 = R_1 + \\gamma G_1 = 2 + 0.9*70 = 65\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 3.10</b> \n",
    "</p>\n",
    "\n",
    "Prove the second equality in (3.10).\n",
    "\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.10 is: \n",
    "$$\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma} \\tag{3.10}\n",
    "$$\n",
    "\n",
    "Let's start with: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_t &= \\sum_{k=0}^{\\infty} \\gamma^k \\\\\n",
    "&= \\frac{(\\sum_{k=0}^{\\infty} \\gamma^k)*(1-\\gamma)}{1-\\gamma} \\\\\n",
    "&= \\frac{1-\\gamma+\\gamma-\\gamma^2+\\gamma^2-\\gamma^3+\\gamma^2-\\gamma^4+\\dots-\\gamma^{\\infty}}{1-\\gamma} \\\\\n",
    "&=\\frac{1-\\cancel{\\gamma}+\\cancel{\\gamma}-\\cancel{\\gamma^2}+\\cancel{\\gamma^2}-\\cancel{\\gamma^3}+\\cancel{\\gamma^3}-\\cancel{\\gamma^4}+\\dots-\\gamma^{\\infty}}{1-\\gamma} \\\\\n",
    "&= \\frac{1-\\gamma^{\\infty}}{1-\\gamma} \\\\\n",
    "\n",
    "\\text{because }\\gamma < 1 : \\\\\n",
    "G_t &= \\frac{1}{1-\\gamma}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
