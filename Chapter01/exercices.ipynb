{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "# Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.1</b> \n",
    "</p>\n",
    "\n",
    "In $\\epsilon$-greedy action selection, for the case of two actions and $\\epsilon = 0.5$, what is\n",
    "the probability that the greedy action is selected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "1-$\\epsilon$ = 50% of the time we will take the greedy action. \n",
    "50% of the time we will take a random action. There are two possible action, only one is greedy. So in the case of making a random choice, we have 50% chance of taking the greedy action. \n",
    "\n",
    "Let's say $P(A)$ = probability of taking the greedy action. $P(B)$ = probability of choosing randomly\n",
    "\n",
    "$P(A)   = P(A)*P(B) + P(A)*P(\\hat{B}) = 0.5*0.5 + 0.5*1 = 0.75$\n",
    "\n",
    "Therefore, there is 75% chance of taking the greedy action in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.2:</b>\n",
    "</p>\n",
    "\n",
    "Bandit example Consider a k-armed bandit problem with $k = 4$ actions,\n",
    "denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using\n",
    "$\\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates\n",
    "of $Q_1(a) = 0$, for all a. Suppose the initial sequence of actions and rewards is $A_1 = 1,\n",
    "R_1 = - 1, A_2 = 2, R_2 = 1, A_3 = 2, R_3 = 2, A_4 = 2, R_4 = 2, A_5 = 3, R_5 = 0$. On some\n",
    "of these time steps the $\\epsilon$ case may have occurred, causing an action to be selected at\n",
    "random. On which time steps did this definitely occur? On which time steps could this\n",
    "possibly have occurred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "On the step number 4, actions 3 and 4 had higher action-value estimates : 0, while action 1 had -1 as action-value estimate, and action 2 had -0.5: $\\frac{(1-2)}{2}$. But action 2 was selected again, so we know the $\\epsilon$ case occured. \n",
    "\n",
    "On step number 5, the value of action 2 was higher than every other action: $Q_5(2) = \\frac{1-2+2}{3} = 0.5$. However, action number 3 was chosen, so we know the $\\epsilon$ case occured again. \n",
    "\n",
    "It could have occured on time step number 1,2 & 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.3</b>\n",
    "</p>\n",
    "\n",
    "In the comparison shown in Figure 2.2, which method will perform best in\n",
    "the long run in terms of cumulative reward and probability of selecting the best action?\n",
    "How much better will it be? Express your answer quantitatively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "By the law of large numbers, as $t \\to \\infty, Q_t(a) \\approx Q_*(a)$\n",
    "\n",
    "So every exploratory method will find the optimal action-value pairs for each action. \n",
    "\n",
    "The $\\epsilon=0.01$ will endup taking the best action $99\\%$ of the time\n",
    "\n",
    "The $\\epsilon=0.1$ will endup taking the best action $90\\%$ of the time\n",
    "\n",
    "The best policy will therefore be $\\frac{0.99}{0.9}=1.1$ times better, $10\\%$ better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.4</b>\n",
    "</p>\n",
    "\n",
    "If the step-size parameters, $\\alpha_n$, are not constant, then the estimate $Q_n$ is\n",
    "a weighted average of previously received rewards with a weighting different from that\n",
    "given by (2.6). What is the weighting on each prior reward for the general case, analogous\n",
    "to (2.6), in terms of the sequence of step-size parameters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{n+1} &= Q_n+\\\\alpha_n[R_n-Q_n] \\\\\n",
    "&= \\alpha_n R_n + (1-\\alpha_n)Q_n \\\\\n",
    "&= \\alpha R_n + (1-\\alpha_n)[\\alpha_{n-1}R_{n-1} + (1-\\alpha_{n-1})Q_{n-1}] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha_n)(\\alpha_{n-1}R_{n-1}) + (1-\\alpha_n)(1-\\alpha_{n-1})Q_{n-1} \\\\\n",
    "&= \\alpha R_n + (1-\\alpha_n)(\\alpha_{n-1}R_{n-1}) + (1-\\alpha_n)(1-\\alpha_{n-1})[\\alpha_{n-2}R_{n-2} + (1-\\alpha_{n-2})Q_{n-2}] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha_n)(\\alpha_{n-1}R_{n-1}) + (1-\\alpha_n)(1-\\alpha_{n-1})\\alpha_{n-2}R_{n-2} + \\\\\n",
    "& \\quad \\cdots + \\prod_{i=1}^{n}(1-\\alpha_i)\\alpha_1R_1 + \\sum_{i=1}^{n}(1-\\alpha_i)Q_1 \\\\\n",
    "&= \\prod_{i=1}^{n}(1-\\alpha_i)Q_1 + \\sum_{i=1}^{n}(\\prod_{j=i}^{n}(1-\\alpha_j))\\alpha_i R_i \\\\\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.6</b>\n",
    "</p>\n",
    "\n",
    " Mysterious Spikes The results shown in Figure 2.3 should be quite reliable\n",
    "because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks.\n",
    "Why, then, are there oscillations and spikes in the early part of the curve for the optimistic\n",
    "method? In other words, what might make this method perform particularly better or\n",
    "worse, on average, on particular early steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "In early steps every agent is greedily trying out the Q=5 actions. Those q_values get updated and they all get a smaller value. if the actual best q_value gets diminished the less, it'll still be chosen for a long time as a greedy action because all the reward are still drawn from a random distribution with mean 0. On the opposite, if the actual best q_values are being drawn the worst rewards at first, it'll take some time before they start being chosen again as greedy actions. As we progress in the stepNumber, those adjustments are smaller and the initial impact of the optimistic Q_values is being less & less. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.7</b>\n",
    "</p>\n",
    "\n",
    "Unbiased Constant-Step-Size Trick: \n",
    "\n",
    "In most of this chapter we have used\n",
    "sample averages to estimate action values because sample averages do not produce the\n",
    "initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample\n",
    "averages are not a completely satisfactory solution because they may perform poorly\n",
    "on nonstationary problems. Is it possible to avoid the bias of constant step sizes while\n",
    "retaining their advantages on nonstationary problems? One way is to use a step size of:\n",
    "\n",
    "$\\beta_n = \\alpha / \\bar{o}_n$ (2.8),\n",
    "\n",
    "to process the nth reward for a particular action, where $\\alpha > 0 $ is a conventional constant\n",
    "step size, and $\\bar{o}_n$ is a trace of one that starts at 0:\n",
    "\n",
    "$\\bar{o}_n = \\bar{o}_{n-1} + \\alpha (1-\\bar{o}_{n-1}) \\quad for \\quad n \\geq 0 \\quad \\bar{o}_0 = 0 (2.9)$\n",
    "\n",
    "Carry out an analysis like that in (2.6) to show that Qn is an exponential recency-weighted\n",
    "average without initial bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{n+1} &= Q_n + \\beta_n (R_n - Q_n) \\\\\n",
    "&= \\beta_n R_n + (1-\\beta_n)Q_n \\\\\n",
    "&= \\quad \\cdots \\\\\n",
    "&= \\prod_{i=1}^{n}(1-\\beta_i)Q_1 + \\sum_{i=1}^{n}(\\prod_{j=i}^{n}(1-\\beta_j))\\beta_i R_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The initial biais is held in the $\\prod_{i=1}^{n}(1-\\beta_i)Q_1$ term\n",
    "\n",
    "for $n=1$, this is equal to: \n",
    "\n",
    "$(1-\\beta_{1})Q_1$\n",
    "\n",
    "$\\beta_i = \\alpha/\\bar{o}_i$\n",
    "\n",
    "$\\bar{o}_0 = $ and $\\bar{o}_1 = \\alpha$ \n",
    "\n",
    "therefore, for $n=1$\n",
    "\n",
    "$(1-\\beta_{1})Q_1 = 0$\n",
    "\n",
    "This shows that this doesn't hold any initial biais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
