{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "# Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.1</b> \n",
    "</p>\n",
    "\n",
    "In $\\epsilon$-greedy action selection, for the case of two actions and $\\epsilon = 0.5$, what is\n",
    "the probability that the greedy action is selected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "1-$\\epsilon$ = 50% of the time we will take the greedy action. \n",
    "50% of the time we will take a random action. There are two possible action, only one is greedy. So in the case of making a random choice, we have 50% chance of taking the greedy action. \n",
    "\n",
    "Let's say $P(A)$ = probability of taking the greedy action. $P(B)$ = probability of choosing randomly\n",
    "\n",
    "$P(A)   = P(A)*P(B) + P(A)*P(\\hat{B}) = 0.5*0.5 + 0.5*1 = 0.75$\n",
    "\n",
    "Therefore, there is 75% chance of taking the greedy action in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.2:</b>\n",
    "</p>\n",
    "\n",
    "Bandit example Consider a k-armed bandit problem with $k = 4$ actions,\n",
    "denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using\n",
    "$\\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates\n",
    "of $Q_1(a) = 0$, for all a. Suppose the initial sequence of actions and rewards is $A_1 = 1,\n",
    "R_1 = - 1, A_2 = 2, R_2 = 1, A_3 = 2, R_3 = 2, A_4 = 2, R_4 = 2, A_5 = 3, R_5 = 0$. On some\n",
    "of these time steps the $\\epsilon$ case may have occurred, causing an action to be selected at\n",
    "random. On which time steps did this definitely occur? On which time steps could this\n",
    "possibly have occurred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "On the step number 4, actions 3 and 4 had higher action-value estimates : 0, while action 1 had -1 as action-value estimate, and action 2 had -0.5: $\\frac{(1-2)}{2}$. But action 2 was selected again, so we know the $\\epsilon$ case occured. \n",
    "\n",
    "On step number 5, the value of action 2 was higher than every other action: $Q_5(2) = \\frac{1-2+2}{3} = 0.5$. However, action number 3 was chosen, so we know the $\\epsilon$ case occured again. \n",
    "\n",
    "It could have occured on time step number 1,2 & 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.3</b>\n",
    "</p>\n",
    "\n",
    "In the comparison shown in Figure 2.2, which method will perform best in\n",
    "the long run in terms of cumulative reward and probability of selecting the best action?\n",
    "How much better will it be? Express your answer quantitatively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "By the law of large numbers, as $t \\to \\infty, Q_t(a) \\approx Q_*(a)$\n",
    "\n",
    "So every exploratory method will find the optimal action-value pairs for each action. \n",
    "\n",
    "The $\\epsilon=0.01$ will endup taking the best action $99\\%$ of the time\n",
    "\n",
    "The $\\epsilon=0.1$ will endup taking the best action $90\\%$ of the time\n",
    "\n",
    "The best policy will therefore be $\\frac{0.99}{0.9}=1.1$ times better, $10\\%$ better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 2.4</b>\n",
    "</p>\n",
    "\n",
    "If the step-size parameters, $\\alpha_n$, are not constant, then the estimate $Q_n$ is\n",
    "a weighted average of previously received rewards with a weighting different from that\n",
    "given by (2.6). What is the weighting on each prior reward for the general case, analogous\n",
    "to (2.6), in terms of the sequence of step-size parameters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{n+1} &= Q_n+\\alpha_n[R_n-Q_n] \\\\\n",
    "&= \\alpha_n R_n + (1-\\alpha_n)Q_n \\\\\n",
    "&= \\alpha R_n + (1-\\alpha_n)[\\alpha_{n-1}R_{n-1} + (1-\\alpha_{n-1})Q_{n-1}] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha_n)(\\alpha_{n-1}R_{n-1}) + (1-\\alpha_n)(1-\\alpha_{n-1})Q_{n-1} \\\\\n",
    "&= \\alpha R_n + (1-\\alpha_n)(\\alpha_{n-1}R_{n-1}) + (1-\\alpha_n)(1-\\alpha_{n-1})[\\alpha_{n-2}R_{n-2} + (1-\\alpha_{n-2})Q_{n-2}] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha_n)(\\alpha_{n-1}R_{n-1}) + (1-\\alpha_n)(1-\\alpha_{n-1})\\alpha_{n-2}R_{n-2} + \\\\\n",
    "&\\quad \\cdots + \\prod_{i=1}^{n}(1-\\alpha_i)\\alpha_1R_1 + \\sum_{i=1}^{n}(1-\\alpha_i)Q_1 \\\\\n",
    "&= \\prod_{i=1}^{n}(1-\\alpha_i)Q_1 + \\sum_{i=1}^{n}(\\prod_{j=i}^{n}(1-\\alpha_j))\\alpha_i R_i\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduction of 2.2: \n",
    "\n",
    "![Image description](Fig2.2_simplebandit_repro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
