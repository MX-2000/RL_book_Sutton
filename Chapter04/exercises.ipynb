{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.1</b> \n",
    "</p>\n",
    "\n",
    "In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_{\\pi}(11, down)$? What is $q_{\\pi}(7, down)$? \n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_{\\pi}(11,down) = 1*[-1 + 1*0] = -1 \\\\\n",
    "\n",
    "Q_{\\pi}(7,down) = 1*[-1 + \\sum_{a}\\pi_{a,11}Q_{\\pi}(a,11)] = -1 + V_{\\pi}(11) = -15 \\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.2</b> \n",
    "</p>\n",
    "\n",
    "In Example 4.1, suppose a new state 15 is added to the gridworld just below\n",
    "state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_{\\pi}(15) for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new\n",
    "state 15. What is $v_{\\pi}(15)$ for the equiprobable random policy in this case?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{\\pi}(15) = -1 + \\frac{1}{4}[v_{\\pi}(12) + v_{\\pi}(13) + v_{\\pi}(14) + v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) = -\\frac{4}{3} + \\frac{1}{3}[v_{\\pi}(12) + v_{\\pi}(13) + v_{\\pi}(14)] = \\frac{-4}{3} + \\frac{1}{3}[-22-20-14] = -20\n",
    "$$\n",
    "\n",
    "In the secon case: \n",
    "\n",
    "$$\n",
    "v_{\\pi}(13) = -1 + \\frac{1}{4}[v_{\\pi}(12) + v_{\\pi}(14) + v_{\\pi}(9) + v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(13) = -15 + \\frac{v_{\\pi}(15)}{4} \\\\\n",
    "v_{\\pi}(15) = -1 + \\frac{1}{4}[v_{\\pi}(12) + v_{\\pi}(13) + v_{\\pi}(14) + v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) = \\frac{-40+v_{\\pi}(13)}{3}\n",
    "$$\n",
    "\n",
    "If we plug $v_{\\pi}(13)$ into $v_{\\pi}(15)$ :\n",
    "\n",
    "$$\n",
    "v_{\\pi}(15) = \\frac{-40-15+\\frac{v_{\\pi}(13)}{4}}{3} = -20\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.3</b> \n",
    "</p>\n",
    "\n",
    "What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_{\\pi}$ and its successive approximation by a sequence of functions $q_0, q_1, q_2,$...?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_{k+1} = \\sum_{s',r}p(s',r|a,s)[r + \\gamma \\sum_{a'}\\pi_(a'|s')q_k(a',s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.4</b> \n",
    "</p>\n",
    "\n",
    "The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed.\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "The part of the code that may lead the policy to continually switch is: \n",
    "\n",
    "If olf-action $\\neq \\pi(s)$, then policy-stable $\\leftarrow$ false\n",
    "\n",
    "We can either keep the previous policy in memory for each action and make sure we don't switch back, if we do then we stop. \n",
    "\n",
    "We could break ties in a deterministic manner when choosing argmax, for example, always taking the first index. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.5</b> \n",
    "</p>\n",
    "\n",
    "How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book.\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration (using iterative policy evaluation)\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "    \\text{1. Initialization:} \\\\\n",
    "    \\quad Q(s,a) \\in \\R \\text{ and } \\pi(s) \\in \\mathbb{A}(s) \\text{ arbitrarily for all } s \\in \\mathbb{S} \\text{ and all } a \\in \\mathbb{A}(s) \\\\ \\\\\n",
    "    \n",
    "    \\text{2. Policy Evaluation} \\\\\n",
    "    \\text{Loop:} \\\\\n",
    "        \\quad \\Delta \\leftarrow 0 \\\\\n",
    "        \\quad \\text{Loop for each } s \\in \\mathbb{S}:\\\\\n",
    "            \\quad \\quad \\text{Loop for each } a \\in \\mathbb{A}(s):\\\\\n",
    "                \\quad \\quad \\quad Q(s,a) \\leftarrow \\sum_{s',r}p(s',r|s,a)[r + \\gamma q(s',\\pi(s'))] \\\\\n",
    "            \\quad \\quad q \\leftarrow max_a(Q(s,a)) \\\\\n",
    "            \\quad \\quad \\Delta \\leftarrow max(\\Delta, |q - Q(s,\\pi(s))|) \\\\\n",
    "\n",
    "    \\text{until } \\Delta < \\theta \\text{ (a small positive number determining the accuracy of estimation)} \\\\ \\\\\n",
    "\n",
    "    \\text{3. Policy Improvment} \\\\\n",
    "        \\quad policy\\text{-}stable \\leftarrow true \\\\\n",
    "        \\quad \\text{For each } s \\in \\mathbb{S}: \\\\\n",
    "            \\quad \\quad old\\text{-}action \\leftarrow \\pi(s) \\\\\n",
    "            \\quad \\quad \\pi(s) \\leftarrow argmax_a Q(s,a) \\text{(taking the max deterministically)} \\\\\n",
    "            \\quad \\quad \\text{If } old\\text{-}action \\neq \\pi(s)\\text{, then } policy\\text{-}stable \\leftarrow false \\\\\n",
    "        \\text{If } policy\\text{-}stable \\text{, then stop and return } Q \\approx Q_* \\text{ and } \\pi \\approx \\pi_* \\text{; else go to 2}\n",
    "            \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.6</b> \n",
    "</p>\n",
    "\n",
    "Suppose you are restricted to considering only policies that are $\\epsilon$-soft,\n",
    "meaning that the probability of selecting each action in each state, $s$, is at least $\\epsilon/|A(s)|$.\n",
    "Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1,\n",
    "in that order, of the policy iteration algorithm for $v_*$ on page 80.\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are basically changing from a deterministic policy to a stochastic one.\n",
    "\n",
    "In step 1, we need to initialize $\\pi(s)$ with probabilities $> \\epsilon/|A(s)|$ for each $a$\n",
    "\n",
    "In step 2, we need to update the $V(s)$ expression to include the sum over all probabilities of the policy taking the action (goes bas to original Bellman equation for the general case for $V(s)$)\n",
    "\n",
    "In step 3, we need to make sure that eacy $\\pi(s,a)$ satisfies the rule of having a probability of being taken of $\\epsilon/|A(s)|$. So, while looping over all a, all action probabilities still need to sum up to 1. \n",
    "\n",
    "$$\n",
    "\\pi(a,s) = \n",
    "\\begin{cases}\n",
    "1-\\epsilon + \\epsilon/|A(s)|, & \\text{if } a=argmax_a'q(a',s)\\\\\n",
    "\\epsilon/|A(s)|, & \\text{else}\n",
    "\\end{cases}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.8</b> \n",
    "</p>\n",
    "\n",
    "Why does the optimal policy for the gamblerâ€™s problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "Betting everything on 51 would be a waste of one chip because doubling it wouldn't take full advantage as the goal is to reach 100$ only. In case of failure, it would resut in losing where betting only 50 and failure would lead to having 1$ left. \n",
    "\n",
    "At 50, there is 40% chance of winning the game right here. Same for 75. For 25, the goal is to double and go to 50 if one go, and the strategy is only betting the entire stack when it can win. \n",
    "\n",
    "The fact that rewards are undiscounted and no negative rewards are given for failure or bets, whether the agent wins the game after 100 bets or 1 bets doesn't change anything. The agent is therefore incentivized to minimize its chance of failure, even if it might take him much longer. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
