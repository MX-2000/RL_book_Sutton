{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.1</b> \n",
    "</p>\n",
    "\n",
    "In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_{\\pi}(11, down)$? What is $q_{\\pi}(7, down)$? \n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_{\\pi}(11,down) = 1*[-1 + 1*0] = -1 \\\\\n",
    "\n",
    "Q_{\\pi}(7,down) = 1*[-1 + \\sum_{a}\\pi_{a,11}Q_{\\pi}(a,11)] = -1 + V_{\\pi}(11) = -15 \\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.2</b> \n",
    "</p>\n",
    "\n",
    "In Example 4.1, suppose a new state 15 is added to the gridworld just below\n",
    "state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_{\\pi}(15) for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new\n",
    "state 15. What is $v_{\\pi}(15)$ for the equiprobable random policy in this case?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{\\pi}(15) = -1 + \\frac{1}{4}[v_{\\pi}(12) + v_{\\pi}(13) + v_{\\pi}(14) + v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) = -\\frac{4}{3} + \\frac{1}{3}[v_{\\pi}(12) + v_{\\pi}(13) + v_{\\pi}(14)] = \\frac{-4}{3} + \\frac{1}{3}[-22-20-14] = -20\n",
    "$$\n",
    "\n",
    "In the secon case: \n",
    "\n",
    "$$\n",
    "v_{\\pi}(13) = -1 + \\frac{1}{4}[v_{\\pi}(12) + v_{\\pi}(14) + v_{\\pi}(9) + v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(13) = -15 + \\frac{v_{\\pi}(15)}{4} \\\\\n",
    "v_{\\pi}(15) = -1 + \\frac{1}{4}[v_{\\pi}(12) + v_{\\pi}(13) + v_{\\pi}(14) + v_{\\pi}(15)] \\\\\n",
    "v_{\\pi}(15) = \\frac{-40+v_{\\pi}(13)}{3}\n",
    "$$\n",
    "\n",
    "If we plug $v_{\\pi}(13)$ into $v_{\\pi}(15)$ :\n",
    "\n",
    "$$\n",
    "v_{\\pi}(15) = \\frac{-40-15+\\frac{v_{\\pi}(13)}{4}}{3} = -20\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.3</b> \n",
    "</p>\n",
    "\n",
    "What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_{\\pi}$ and its successive approximation by a sequence of functions $q_0, q_1, q_2,$...?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_{k+1} = \\sum_{s',r}p(s',r|a,s)[r + \\gamma \\sum_{a'}\\pi_(a'|s')q_k(a',s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 4.4</b> \n",
    "</p>\n",
    "\n",
    "The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed.\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>\n",
    "\n",
    "The part of the code that may lead the policy to continually switch is: \n",
    "\n",
    "If olf-action $\\neq \\pi(s)$, then policy-stable $\\leftarrow$ false\n",
    "\n",
    "We can either keep the previous policy in memory for each action and make sure we don't switch back, if we do then we stop. \n",
    "\n",
    "We could break ties in a deterministic manner when choosing argmax, for example, always taking the first index. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
