{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.1</b> \n",
    "</p>\n",
    "\n",
    "If V changes during the episode, then (6.6) only holds approximately; what\n",
    "would the difference be between the two sides? Let $V_t$ denote the array of state values\n",
    "used at time t in the TD error (6.5) and in the TD update (6.2). Redo the derivation\n",
    "above to determine the additional amount that must be added to the sum of TD errors\n",
    "in order to equal the Monte Carlo error.\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_{t+1}(S_{t+1})$ is the same as $V_t(S_{t+1})$ in MC case, and most of the time in TD case. But when $S_{t+1} = S_t$ then, in TD, we update this value in the previous step (because of the TD error incremental update). Therefore, those two terms are not equal, and they differ by the value of the TD error: $\\alpha[R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)]$\n",
    "\n",
    "Because of this, we have almost the same equations, except when $S_t = S_{t+1}$ when we need to account for the term above\n",
    "\n",
    "$V_{t+1}(S_t) = V_t(S_t) + \\alpha[R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)] = V_t(S_t) + \\alpha \\delta_t$\n",
    "\n",
    "\n",
    "$G_t - V(S_t) = R_{t+1} + \\gamma G_{t+1} - V_t(S_t) = \\delta_t + \\gamma[G_{t+1} - V_t(S_{t+1})]$\n",
    "\n",
    "With what we said above: \n",
    "\n",
    "$V_t(S_{t+1}) = V_{t+1}(S_{t+1}) - \\alpha \\delta_t \\mathbb{1}(S_{t+1}=S_t)$ with $\\mathbb{1}$ being the indicator function\n",
    "\n",
    "Therefore, we have: \n",
    "\n",
    "$G_t - V_t(S_t) = \\delta_t + \\gamma \\alpha \\delta_t \\mathbb{1}(S_{t+1}=S_t) + \\gamma[G_{t+1} - V_{t+1}(S_{t+1})] = \\delta_t + \\gamma \\alpha \\delta_t \\mathbb{1}(S_{t+1}=S_t) + \\gamma[\\delta_{t+1} + \\gamma \\alpha \\delta_{t+1}\\mathbb{1}(S_{t+1}=S_t) + \\gamma(G_{t+2} - V_{t+2}(S_{t+2}))]$\n",
    "\n",
    "Finally: \n",
    "\n",
    "$G_t - V_t(S_t) = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k + \\alpha \\gamma \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_t\\mathbb{1}(S_{t+1}=S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.2</b> \n",
    "</p>\n",
    "\n",
    "This is an exercise to help develop your intuition about why TD methods\n",
    "are often more ecient than Monte Carlo methods. Consider the driving home example\n",
    "and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario\n",
    "in which a TD update would be better on average than a Monte Carlo update? Give\n",
    "an example scenario—a description of past experience and a current state—in which\n",
    "you would expect the TD update to be better. Here’s a hint: Suppose you have lots of\n",
    "experience driving home from work. Then you move to a new building and a new parking\n",
    "lot (but you still enter the highway at the same place). Now you are starting to learn\n",
    "predictions for the new building. Can you see why TD updates are likely to be much\n",
    "better, at least initially, in this case? Might the same sort of thing happen in the original\n",
    "scenario?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because MC methods will need entire episodes of old behaviour before learning the new optimal path, and because TD methods will adjust on the fly, TD methods will adjust faster and converge faster, therefore should be better, at least initially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.3</b> \n",
    "</p>\n",
    "\n",
    "From the results shown in the left graph of the random walk example it\n",
    "appears that the first episode results in a change in only V (A). What does this tell you\n",
    "about what happened on the first episode? Why was only the estimate for this one state\n",
    "changed? By exactly how much was it changed?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means the first episode terminated through state A. The new value update for state A became: \n",
    "$V_A \\leftarrow V_a + \\alpha[R + \\gamma V_{\\text{terminal}} - V_a] = 0.5 - 0.1[0+1*0-0.5] = 0.45$\n",
    "\n",
    "The other weren't changed because their updates were: \n",
    "\n",
    "$V_x \\leftarrow V_x + \\alpha[R + \\gamma V_{\\text{next}} - V_a] = 0.5 + 0.1[0 + 1*0.5 - 0.5] = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.4</b> \n",
    "</p>\n",
    "\n",
    "The specific results shown in the right graph of the random walk example\n",
    "are dependent on the value of the step-size parameter, $\\alpha$. Do you think the conclusions\n",
    "about which algorithm is better would be affected if a wider range of $\\alpha$ values were used?\n",
    "Is there a different, fixed value of $\\alpha$ at which either algorithm would have performed\n",
    "significantly better than shown? Why or why not?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With higher $\\alpha$, TD start converging faster but in the end will diverge, and be worse than MC methods for which higher $\\alpha$ means more noise but better RMS error"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
