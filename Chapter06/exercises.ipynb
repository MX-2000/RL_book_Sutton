{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.1</b> \n",
    "</p>\n",
    "\n",
    "If V changes during the episode, then (6.6) only holds approximately; what\n",
    "would the difference be between the two sides? Let $V_t$ denote the array of state values\n",
    "used at time t in the TD error (6.5) and in the TD update (6.2). Redo the derivation\n",
    "above to determine the additional amount that must be added to the sum of TD errors\n",
    "in order to equal the Monte Carlo error.\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_{t+1}(S_{t+1})$ is the same as $V_t(S_{t+1})$ in MC case, and most of the time in TD case. But when $S_{t+1} = S_t$ then, in TD, we update this value in the previous step (because of the TD error incremental update). Therefore, those two terms are not equal, and they differ by the value of the TD error: $\\alpha[R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)]$\n",
    "\n",
    "Because of this, we have almost the same equations, except when $S_t = S_{t+1}$ when we need to account for the term above\n",
    "\n",
    "$V_{t+1}(S_t) = V_t(S_t) + \\alpha[R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)] = V_t(S_t) + \\alpha \\delta_t$\n",
    "\n",
    "\n",
    "$G_t - V(S_t) = R_{t+1} + \\gamma G_{t+1} - V_t(S_t) = \\delta_t + \\gamma[G_{t+1} - V_t(S_{t+1})]$\n",
    "\n",
    "With what we said above: \n",
    "\n",
    "$V_t(S_{t+1}) = V_{t+1}(S_{t+1}) - \\alpha \\delta_t \\mathbb{1}(S_{t+1}=S_t)$ with $\\mathbb{1}$ being the indicator function\n",
    "\n",
    "Therefore, we have: \n",
    "\n",
    "$G_t - V_t(S_t) = \\delta_t + \\gamma \\alpha \\delta_t \\mathbb{1}(S_{t+1}=S_t) + \\gamma[G_{t+1} - V_{t+1}(S_{t+1})] = \\delta_t + \\gamma \\alpha \\delta_t \\mathbb{1}(S_{t+1}=S_t) + \\gamma[\\delta_{t+1} + \\gamma \\alpha \\delta_{t+1}\\mathbb{1}(S_{t+1}=S_t) + \\gamma(G_{t+2} - V_{t+2}(S_{t+2}))]$\n",
    "\n",
    "Finally: \n",
    "\n",
    "$G_t - V_t(S_t) = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k + \\alpha \\gamma \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_t\\mathbb{1}(S_{t+1}=S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.2</b> \n",
    "</p>\n",
    "\n",
    "This is an exercise to help develop your intuition about why TD methods\n",
    "are often more ecient than Monte Carlo methods. Consider the driving home example\n",
    "and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario\n",
    "in which a TD update would be better on average than a Monte Carlo update? Give\n",
    "an example scenario—a description of past experience and a current state—in which\n",
    "you would expect the TD update to be better. Here’s a hint: Suppose you have lots of\n",
    "experience driving home from work. Then you move to a new building and a new parking\n",
    "lot (but you still enter the highway at the same place). Now you are starting to learn\n",
    "predictions for the new building. Can you see why TD updates are likely to be much\n",
    "better, at least initially, in this case? Might the same sort of thing happen in the original\n",
    "scenario?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because MC methods will need entire episodes of old behaviour before learning the new optimal path, and because TD methods will adjust on the fly, TD methods will adjust faster and converge faster, therefore should be better, at least initially"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
