{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.1</b> \n",
    "</p>\n",
    "\n",
    "If V changes during the episode, then (6.6) only holds approximately; what\n",
    "would the difference be between the two sides? Let $V_t$ denote the array of state values\n",
    "used at time t in the TD error (6.5) and in the TD update (6.2). Redo the derivation\n",
    "above to determine the additional amount that must be added to the sum of TD errors\n",
    "in order to equal the Monte Carlo error.\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_{t+1}(S_{t+1})$ is the same as $V_t(S_{t+1})$ in MC case, and most of the time in TD case. But when $S_{t+1} = S_t$ then, in TD, we update this value in the previous step (because of the TD error incremental update). Therefore, those two terms are not equal, and they differ by the value of the TD error: $\\alpha[R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)]$\n",
    "\n",
    "Because of this, we have almost the same equations, except when $S_t = S_{t+1}$ when we need to account for the term above\n",
    "\n",
    "$V_{t+1}(S_t) = V_t(S_t) + \\alpha[R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)] = V_t(S_t) + \\alpha \\delta_t$\n",
    "\n",
    "\n",
    "$G_t - V(S_t) = R_{t+1} + \\gamma G_{t+1} - V_t(S_t) = \\delta_t + \\gamma[G_{t+1} - V_t(S_{t+1})]$\n",
    "\n",
    "With what we said above: \n",
    "\n",
    "$V_t(S_{t+1}) = V_{t+1}(S_{t+1}) - \\alpha \\delta_t \\mathbb{1}(S_{t+1}=S_t)$ with $\\mathbb{1}$ being the indicator function\n",
    "\n",
    "Therefore, we have: \n",
    "\n",
    "$G_t - V_t(S_t) = \\delta_t + \\gamma \\alpha \\delta_t \\mathbb{1}(S_{t+1}=S_t) + \\gamma[G_{t+1} - V_{t+1}(S_{t+1})] = \\delta_t + \\gamma \\alpha \\delta_t \\mathbb{1}(S_{t+1}=S_t) + \\gamma[\\delta_{t+1} + \\gamma \\alpha \\delta_{t+1}\\mathbb{1}(S_{t+1}=S_t) + \\gamma(G_{t+2} - V_{t+2}(S_{t+2}))]$\n",
    "\n",
    "Finally: \n",
    "\n",
    "$G_t - V_t(S_t) = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k + \\alpha \\gamma \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_t\\mathbb{1}(S_{t+1}=S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.2</b> \n",
    "</p>\n",
    "\n",
    "This is an exercise to help develop your intuition about why TD methods\n",
    "are often more ecient than Monte Carlo methods. Consider the driving home example\n",
    "and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario\n",
    "in which a TD update would be better on average than a Monte Carlo update? Give\n",
    "an example scenario—a description of past experience and a current state—in which\n",
    "you would expect the TD update to be better. Here’s a hint: Suppose you have lots of\n",
    "experience driving home from work. Then you move to a new building and a new parking\n",
    "lot (but you still enter the highway at the same place). Now you are starting to learn\n",
    "predictions for the new building. Can you see why TD updates are likely to be much\n",
    "better, at least initially, in this case? Might the same sort of thing happen in the original\n",
    "scenario?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because MC methods will need entire episodes of old behaviour before learning the new optimal path, and because TD methods will adjust on the fly, TD methods will adjust faster and converge faster, therefore should be better, at least initially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.3</b> \n",
    "</p>\n",
    "\n",
    "From the results shown in the left graph of the random walk example it\n",
    "appears that the first episode results in a change in only V (A). What does this tell you\n",
    "about what happened on the first episode? Why was only the estimate for this one state\n",
    "changed? By exactly how much was it changed?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means the first episode terminated through state A. The new value update for state A became: \n",
    "$V_A \\leftarrow V_a + \\alpha[R + \\gamma V_{\\text{terminal}} - V_a] = 0.5 - 0.1[0+1*0-0.5] = 0.45$\n",
    "\n",
    "The other weren't changed because their updates were: \n",
    "\n",
    "$V_x \\leftarrow V_x + \\alpha[R + \\gamma V_{\\text{next}} - V_a] = 0.5 + 0.1[0 + 1*0.5 - 0.5] = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.4</b> \n",
    "</p>\n",
    "\n",
    "The specific results shown in the right graph of the random walk example\n",
    "are dependent on the value of the step-size parameter, $\\alpha$. Do you think the conclusions\n",
    "about which algorithm is better would be affected if a wider range of $\\alpha$ values were used?\n",
    "Is there a different, fixed value of $\\alpha$ at which either algorithm would have performed\n",
    "significantly better than shown? Why or why not?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With higher $\\alpha$, TD start converging faster but in the end will diverge, and be worse than MC methods for which higher $\\alpha$ means more noise but better RMS error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.5</b> \n",
    "</p>\n",
    "\n",
    "In the right graph of the random walk example, the RMS error of the\n",
    "TD method seems to go down and then up again, particularly at high $\\alpha$’s. What could\n",
    "have caused this? Do you think this always occurs, or might it be a function of how the\n",
    "approximate value function was initialized?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "high $\\alpha$ lead to a faster convergence and drop in RMSE but later to a noisier estimations due to too much weight being given to the new updates and noise. This always occur as long as states are initialized with the same values. Therefore, a random initialization of the state values is to be considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:25px;\">\n",
    "<b>Exercise 6.6</b> \n",
    "</p>\n",
    "\n",
    "In Example 6.2 we stated that the true values for the random walk example\n",
    "are $\\frac{1}{6}, \\frac{2}{6}, \\frac{3}{6}, \\frac{4}{6} \\text{ and } \\frac{5}{6}$, for states A through E. Describe at least two different ways that\n",
    "these could have been computed. Which would you guess we actually used? Why?\n",
    "\n",
    "<p style=\"font-size:22px;\">\n",
    "<b>Answer:</b> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either use conditionnal probabilities or solving the Bellman equations:\n",
    "\n",
    "$V_{\\pi}(s) = \\sum_{a}\\pi(a,s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma V_{\\pi}(s')]$\n",
    "\n",
    "Because the transition probabilities are 1, and probability of choosing any of the two actions is always 0.5. No discounting, so Bellman equation becomes: \n",
    "\n",
    "$V_{\\pi}(s) = \\sum_{a}0.5[r+V_{\\pi}(s')]$\n",
    "\n",
    "And for each state we have: \n",
    "\n",
    "$V(A) = 0.5[0+V(Terminal)] + 0.5[0 + V(B)] = 0.5V(B)\\\\$\n",
    "$V(A) - 0.5V(B) = 0$\n",
    "\n",
    "$V(B) = 0.5[0+V(A)] + 0.5[0 + V(C)] = 0.5V(A) + 0.5V(C)\\\\$\n",
    "$V(B) - 0.5V(A) - 0.5V(C) = 0$\n",
    "\n",
    "$V(C) = 0.5[0+V(B)] + 0.5[0 + V(D)] = 0.5V(B) + 0.5V(D)\\\\$\n",
    "$V(C) - 0.5V(B) - 0.5V(D) = 0$\n",
    "\n",
    "$V(D) = 0.5[0+V(C)] + 0.5[0 + V(E)] = 0.5V(C) + 0.5V(E)\\\\$\n",
    "$V(D) - 0.5V(C) - 0.5V(E) = 0$\n",
    "\n",
    "$V(E) = 0.5[0+V(D)] + 0.5[1 + V(Terminal)] = 0.5V(D) + 0.5\\\\$\n",
    "$V(E) - 0.5V(D) = 0.5$\n",
    "\n",
    "Therefore we have a linear system of equations: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & -0.5 & 0 & 0 & 0 \\\\\n",
    "-0.5 & 1 & -0.5 & 0 & 0 \\\\\n",
    "0 & -0.5 & 1 & -0.5 & 0 \\\\\n",
    "0 & 0 & -0.5 & 1 & -0.5 \\\\\n",
    "0 & 0 & 0 & -0.5 & 1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "V(A) \\\\\n",
    "V(B) \\\\\n",
    "V(C) \\\\\n",
    "V(D) \\\\\n",
    "V(E)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sovling it using numpy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667],\n",
       "       [0.33333333],\n",
       "       [0.5       ],\n",
       "       [0.66666667],\n",
       "       [0.83333333]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "M = np.array([[1,-0.5,0,0,0],[-0.5,1,-0.5,0,0],[0,-0.5,1,-0.5,0],[0,0,-0.5,1,-0.5],[0,0,0,-0.5,1]], dtype=np.float64)\n",
    "B = np.array([0,0,0,0,0.5], dtype=np.float64).reshape((-1,1))\n",
    "\n",
    "X = np.linalg.solve(M,B)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the values found. \n",
    "\n",
    "I guess the authors solved it using Bellman equations was used because the first one seems much more complicated. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
